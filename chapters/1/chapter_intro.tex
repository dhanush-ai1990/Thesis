\startfirstchapter{Introduction}
\label{chapter:introduction}

Semantics is the branch of linguistics which is concerned with the study of language and how we understand the meaning of words and concepts. Cognitive Psychologists study semantics to understand various mechanisms that are involved in the thought process and mental representations which are fundamental to any language. 
Semantics is studied across all the three major representations of language -The Brain, Images, and Text. Researchers in the past have performed various studies to understand the similarities between semantic features across all the three representations. The results from these studies have large implications in the field of Computation Linguistics and Artificial Intelligence (AI). For example study of image features and it's semantics could help us build better computer systems that are capable of performing object recognition such as those used in self-driving cars. 

Distributional Semantic Models (DSM) that are based on text corpora (commonly referred as word vectors) have been widely used in the field of Natural Language Processing (NLP) for tasks such as document categorization, speech to text and machine translation. These models are based on the occurrence of words in large corpora of text. The main intuition behind these models is that words which are used in the same context in texts could have some similarities with each other. In these semantic models, the meaning of a word is represented by a vector which approximates the relative position of a word in a large high dimensional space. The words which are used in the same context are positioned closer to one another in the vector space. Due to their importance in the field of AI and widespread adoption by the researchers and industry alike, it becomes really important to test and evaluate the performance of these word vectors. Some of the popular DSM's are discussed under the related work chapter of this work.

\section{The Study of Human Brain}
Computational linguists have also adopted the semantic models based on text to study semantic representation in the human brain. The study of the human brain helps us to gain knowledge about how language is processed and interpreted by us. Over the last decade, we have made significant process in unlocking the complex process through which our brain decodes and understand the meaning of various concepts. This could be credited to the development of various brain imaging techniques such as Functional Magnetic Resonance Imaging (fMRI), Electro-Encephalogram (EEG), Magnetoencephalography (MEG) etc. Linguists studying brain often use text-based semantic models as ground truth in their research. The primary reason for such adoption is that such models are easier to construct and use in the experiments as compared to semantic models based on Brain imaging data or images.

Brain imaging technologies such as fMRI, EEG, and MEG are expensive and constructing semantic models based on these techniques are not feasible. Moreover, the time required to collect enough brain data to build semantic models is extensive as well. It is not feasible to construct models based on images either since collection and storage of large database of images is not an easy task. The extraction of semantic features from images are generally computationally expensive. These reasons could explain why Distributional semantic models based on the text are widely adopted by researchers to study other representations of language.

The inception of such studies began with Michell et al. (2008) who were the first to use text-based semantic models to study semantic features of concepts in human brain collected using fMRI \cite{Mitchell1191}. Murphy et al. (2009) showed that corpus-based semantic representations can predict EEG activity with higher accuracy \cite{MurphyEEG}. This was followed by Sudre et al. (2012) who performed similar work using MEG data collected from subjects viewing images of concrete nouns \cite{SUDRE2012451}.  They also evaluated the performance of various corpus-based models on their task. These works indicate that the semantic representation extracted from corpus could contribute to the study of the brain. 

\section{Introduction to BrainBench}
Anderson et al. argued that a strong correlation between the neural signal and corpus-based models \cite{Mitchell1191, MurphyEEG, SUDRE2012451}  is a good indicator that brain data could be used to test corpus-based models \cite{andersonBrainEyes}. If a corpus-based model could approximate and predict the semantic feature representation in the brain, then that model is said to have features that could represent the ways in which humans learn and understand language. Similar arguments were made by Murphy et al. (2012) \cite{Murphy2012} and Anderson et al. (2015) \cite{Anderson2015}. Based on these recommendations, Xu et al. (2016) introduced \textit{BrainBench}- a system designed to test corpus-based distributional models of semantic using brain data \cite{BrainBench2016}. 

BrainBench used two brain image datasets - a fMRI dataset \cite{Mitchell1191} and a MEG dataset \cite{SUDRE2012451} collected from nine participants imagining 60 concrete nouns. Concrete nouns (eg: Car, Apple etc.) are things which can be experienced by the five senses whereas abstract nouns are intangible concepts such as ideas and emotions (eg. freedom, happy). They tested six popular word vectors against the fMRI and MEG datasets and reported comparable performance to other systems which evaluate and benchmark DSM's. Another notable feature of BrainBench was that it is fast and computationally cheaper as compared to other existing methods. 

However, BrainBench tests include just 60 concrete nouns.  Nouns only form a small subset of various parts of speech that constitutes the language. Another limitation is that the tests do not include any abstract nouns. This could imply that the BrainBench tests could be more biased towards word vectors which has a higher distribution of concrete nouns over abstract. Moreover, the tests are derived from only two dataset sources and based on one language (English). Considering that DSM's are available in multiple languages and not including brain data from other language sources as a part of BrainBench tests is a serious limitation. These problems are summarized in the section \textit{problem statements} of this chapter.


%Talk about Convolutional Neural networks

\section{Semantic representation in Images}

The human visual cortex helps us to interpret the visual stimulus and derive semantics from our sight. In 1962, Hubel and Wiesel studied the perceptual system in cats and concluded that the visual cortex in the brain is made up of special arrangements of cells which are sensitive to only specific regions in the visual field \cite{Hubel992}. These cells or neurons only fire in the presence of edges of certain orientations or patches of color. They act as local filters and are tiled across the entire field of view. This process where a filter is moved across the entire field of view is called as convolution. They also identified other complex cells which have a much larger receptive field and takes input from the cells which act as local filters. They, in turn, are connected to other cells with an even large receptive field. This results in a columnar arrangement of cells and results in visual perception. The semantic representation in images could be studied with the help of Convolutional Neural Networks (CNN).

Convolutional Neural Networks is a type of Artificial Neural Networks (ANN) loosely inspired by the human visual cortex.  They are quite popular among the computer vision community and are used for image recognition. Just like the brain, they also contain neurons (perceptron) which work together to form filters. These filters are slid across the entire image and this results in feature maps. This sliding operation is commonly referred to as convolution. The first layer of any CNN is a convolutional block which takes an image as input. The neurons in the beginning layers of a CNN learn low-level features such as edges, orientation, color patches etc. The low-level features learned by the initial layers are given as input to latter layers which learns high-level feature representations. Based on these high-level features, the CNN's are able to identify and label objects correctly  \cite{CNNREF1}\cite{CNNREF2}.

Prior to Convolutional Neural Networks, classical computer vision techniques such as the Na√Øve Bayes classifier using a bag of visual features \cite{Csurka2004}, hierarchal Bayesian models for object categorization \cite{SivicREZF05} and many others were used for object recognition. These methods required a lot of preprocessing and extraction of handcrafted features such as Histogram of Oriented Gradients (HOG) or Scale-invariant Feature Transform (SIFT) from the images before they could be trained to predict images. However, in CNN's the features are learned automatically by the filters and the input image requires very little pre-processing. CNN's also outperformed other classical methods in various image recognition tasks which led to their rapid adoption by researchers and industry. 

\textit{Neocognitron}, the first CNN developed in 1982 by  Kunihiko Fukushima used a hierarchical multiple layer architecture \cite{FukushimaM82}.  The Neocognitron had the ability to recognize various patterns in images based on the difference in their shapes. This was followed by LeNet architecture developed by Yann LeCun of Bell labs who demonstrated that Convolutional Neural Networks could be used for recognition of handwritten digits \cite{LeNet1989}. The development of faster computers and Graphical Processing Units (GPU) created a revolution in the field of CNN's.  A major limitation of CNN is that it requires a large number of labeled images in order to learn from various patterns in images and make successful predictions. 

In 2009, Fei-Fei et al. released \textit{ImageNet} - a free database of 14 million images of 1000 categories, collected and labeled using the Internet \cite{ImageNet2009}. This lead to the genesis of ImageNet Large Scale Visual Recognition Competition (ILSVRC)-  a benchmark competition for object category classification from images \cite{ImageNETChallenge}. Equipped with the processing power of GPU and labeled training images from ImageNet, Krizhevsky et al. released \textit{AlexNet} which achieved a top 5 test error rate of 15.4\% in the ILSVRC, 2012 competition becoming the state of the art in the field of object recognition \cite{Alexnet2012}. A top 5 error could be defined as the rate at which the model does not predict the correct label in its top 5 predictions. 

\textit{AlexNet}  had an 8 layer architecture and one of the deepest network created at that time. It's success set a trend among the research community that having more number of convolutional blocks and hidden layers in the architecture could somehow improve performance. In short, to get better performance, you need to have a deeper architecture with more convolutional layers. The term \textit{deep learning} was coined to represent creation and study such large networks.

%The black box nature of Convolutional Nets
\section{Black Box Convolutional Neural Networks and it's Challenges}

The trend of going deeper with Convolutional Neural network has lead to the creation of a myriad of black box architectures which works well at the task of object classifications. Yet, the authors of these architectures have often failed to provide any explanation why their networks perform better than the other. There is a lack of understanding about why they perform so well or how could they be improved \cite{CNNVisual1}. There has been some interest in the computer vision community to unlock the secrets of CNN's and to provide insights into the high performance of these networks. Such insights could help us to train faster and more robust CNN's. \textit{How does Convolutional Networks see the world ?, What are the features learned by these networks ?, or what makes one network architecture better than the other ?} are some of the questions asked by the deep learning community.

Researchers have focused on methods such as visualization of features of different layers of CNN \cite{CNNVisual1, CNNVisual2, CNNVisual3} or even mathematical models \cite{CNNVisual4} to diagnose and improve the performance of CNN. However, we believe that studying the semantic representation instead of feature representation through the layers of CNN could provide us with more insights. Convolutional Neural Networks learn semantics from the images. An image is nothing but a representation of a real world phenomenon and we believe that the features learned by the CNN could correlate with the semantics of the image.
  

The convergence of semantic information in these networks could be studied with the help of word vectors following the same methodologies used to study semantics in human brain. We propose to use same distributed semantic models that have been used successfully to study semantic representation in the brain for this task \cite{Mitchell1191, MurphyEEG, SUDRE2012451}. The study of semantic representation through the layers of CNN could help us understand how various CNN architecture differs from another and could potentially provide us with a methodology to improve them.


%Problem statements
\section{Problem Statement}

Distributional Semantic Models (DSM) trained on text corpora have been widely used in the field of Natural Language Processing (NLP) for tasks such as document categorization, speech to text and machine translation. They have also been adopted by the Computational linguists to study semantic representation in the human brain. \textit{BrainBench}- a system designed to test corpus-based distributional models of semantic using brain data \cite{BrainBench2016}, reported comparable performance to other systems which evaluate and benchmark DSM's. Another notable feature of BrainBench was that it is fast and computationally cheaper as compared to other existing methods. However, BrainBench tests include just 60 concrete nouns.  Nouns only form a small subset of various parts of speech that constitutes the language. Another limitation is that the tests do not include any abstract nouns. This could imply that the BrainBench tests could be more biased towards word vectors which has a higher distribution of concrete nouns over abstract. Moreover, the tests are derived from only two dataset sources (a fMRI and a MEG) and based on one language (English). Considering that DSM's are available in multiple languages and not including brain data from other language sources as a part of BrainBench tests is a serious limitation.

Convolutional Neural Networks (CNN) is a type of Artificial Neural Networks (ANN) loosely inspired by the human visual cortex and have become the state of the art technology in object recognition from images. Over the last five years, the deep learning community has demonstrated that having more number of convolutional blocks and hidden layers has a direct correlation with the performance of these networks. The trend of going deeper with Convolutional Neural network has lead to the creation of a myriad of black box architectures which works well at the task of object classifications. However, is a lack of understanding or explanation on why these 

 


\section{Terminology}

In this section, we describe the various terminologies used throughout this thesis. Distributional Semantic Models (DSM) trained on text corpora are based on the occurrence of words in large corpora of text. In these models, the meaning of a word is represented by a vector which approximates the relative position of a word with respect to every other word in the corpus based on its similarity with one another. The terms Distributional Semantic Models, word vectors or even semantic models are used interchangeably through out this thesis.

In Machine Learning (ML), a classification task is defined as a task of predicting discrete classes or labels by an algorithm based on some derived features. Some examples of discrete classes include concepts such as animals, tools, furniture etc. A Convolutional Neural Network (CNN)is a deep neural network which takes an image as input and assigns a category or label to the image. A class could be defined as category or label assigned by a machine learning algorithm based on features learned from the input attributes.

Finally, the terms features and attributes that are used throughout this thesis have different meanings. Features are defined as the internal representation of data which is generated by a model or algorithm. An attribute is defined as some properties which describe a concept. For example, a CNN might look for attributes such as the presence of grass, beak or wings in an image to derive features which could enable it to predict the category of an image as a bird. And to be specific, a semantic feature would mean attributes which help us to identify a concept. An example would be for a concept like \textit{orange}, its semantics could be inferred from some of its attributes such as  it's shape, color, size, or properties such as is it edible, is a type of fruit etc. 
















\section{Thesis Organization}

\begin{description}
\item[\textbf{Chapter 1}] includes a brief introduction to the world of semantics, it's various representations and description of the problem statements.
\item[\textbf{Chapter 2}] talks about related work in this area of research and provides a detailed walk through of all the major contributions made in this field. 
\item[\textbf{Chapter 3}] talks about contributions made to BrainBench including the addition of two new datasets and study of semantic features within various regions of human brain.
\item[\textbf{Chapter 4}] describes the study of semantic representations  through the layers of popular convolutional neural networks with results.
\item[\textbf{Chapter 5}] summarizes the results from chapter 3 and chapter 4. It also offers a detailed discussion on the results.
\item[\textbf{Chapter 6}] gives a summary of the problem statements and the contribution of this thesis in solving those problems. We also discuss various future improvements in this area of research.
\end{description}


